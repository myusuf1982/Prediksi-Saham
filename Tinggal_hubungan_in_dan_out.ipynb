{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO673bbV9KPFf4YmSaplfTE"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xvcpqoQkaGR",
        "outputId": "d8b19264-4ee7-4603-8ce3-2d132ee802e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.layers import Input, LSTM, Concatenate, Dense\n",
        "from keras.models import Model\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data preparation\n",
        "data = pd.read_csv(\"/content/drive/My Drive/GOOG-year.csv\")\n",
        "#data =data[:50] # Juml row maksimal yang mau diambil sample 15 row\n",
        "\n",
        "# memilih kolom open dan close saja\n",
        "#data = data[['Open', 'Close','High','Low','Adj Close']] # all = Tanggal,Open,High,Low,Close,Adj Close,Volume\n",
        "data = data[['Open']]\n",
        "\n",
        "data.head()\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "data = scaler.fit_transform(data)\n",
        "#print(data)"
      ],
      "metadata": {
        "id": "v5bz2NpOBfih"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# membuat dataset dengan 90% data training dan 10% data testing\n",
        "train_size = int(len(data) * 0.8) #226 total 252\n",
        "train_data = data[:train_size] # 90% = 226 -252 = 26\n",
        "test_data = data[train_size:]\n",
        "\n",
        "# membuat dataset dengan input dan output\n",
        "def create_dataset(data, look_back=1):\n",
        "    dataX, dataY = [], []\n",
        "    for i in range(len(data) - look_back): # print(len(data)) =252\n",
        "        dataX.append(data[i:(i + look_back),:])\n",
        "        dataY.append(data[i + look_back, :])\n",
        "        \n",
        "    #return np.array(dataX), np.array(dataY)\n",
        "    return np.array(dataX), np.array(dataY)\n",
        "\n",
        "# membuat dataset dengan input 2 hari sebelumnya dan output hari berikutnya\n",
        "look_back = 2\n",
        "trainX, trainY = create_dataset(train_data, look_back) #226,2\n",
        "testX, testY = create_dataset(test_data, look_back)   #252,2\n",
        "#print(trainX)"
      ],
      "metadata": {
        "id": "KdHWNieHkhe9"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# membuat dataset dengan 90% data training dan 10% data testing\n",
        "train_size = int(len(data) * 0.9) #226 total 252\n",
        "train_data = data[:train_size] # 90% = 226 -252 = 26  /Train 90%\n",
        "test_data = data[train_size:]                        #/ Test 10%\n",
        "\n",
        "original_array=np.array(train_data) #trainX & trainY\n",
        "train_data = original_array.ravel()   \n",
        "#print(train_data)\n",
        "\n",
        "original_array1=np.array(test_data) #testX & testY\n",
        "test_data = original_array1.ravel()\n",
        "\n",
        "\n",
        "#input data\n",
        "open_data =train_data # data['Open'].values\n",
        "close_data = train_data # data['Close'].values\n",
        "high_data = train_data # data['High'].values\n",
        "low_data = train_data # data['Low'].values\n",
        "volume_data = train_data #  data['Volume'].values\n",
        "\n",
        "#reshape data\n",
        "open_data = open_data.reshape((len(open_data), 1,1))\n",
        "close_data = close_data.reshape((len(close_data),1, 1))\n",
        "high_data = high_data.reshape((len(high_data), 1,1))\n",
        "low_data = low_data.reshape((len(low_data), 1,1))\n",
        "volume_data = volume_data.reshape((len(volume_data),1, 1))\n",
        "#print(open_data)"
      ],
      "metadata": {
        "id": "FIw9PzZtXK4Z"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#input layer / Setting variable jadi 3 dimensi\n",
        "open_in = Input(shape=(1,1))\n",
        "close_in = Input(shape=(1,1))\n",
        "high_in = Input(shape=(1,1))\n",
        "low_in = Input(shape=(1,1))\n",
        "volume_in = Input(shape=(1,1))\n",
        "\n",
        "#LSTM layer\n",
        "open_lstm = LSTM(50)(open_in)\n",
        "close_lstm = LSTM(50)(close_in)\n",
        "high_lstm = LSTM(50)(high_in)\n",
        "low_lstm = LSTM(50)(low_in)\n",
        "volume_lstm = LSTM(50)(volume_in)\n",
        "\n"
      ],
      "metadata": {
        "id": "Vv8HG2kTkklz"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#concatenate layer\n",
        "merged = Concatenate()([open_lstm, close_lstm, high_lstm, low_lstm, volume_lstm])\n",
        "\n",
        "#output layer\n",
        "output = Dense(1)(merged)\n",
        "\n",
        "#model\n",
        "model = Model([open_in, close_in, high_in, low_in, volume_in], output)\n",
        "model.compile(loss='mean_absolute_error', optimizer='adam')\n",
        "\n",
        "#fit model\n",
        "model.fit([open_data, close_data, high_data, low_data, volume_data], open_data, epochs=5, batch_size=32)\n",
        "#model.fit([open_data, close_data, high_data, low_data, volume_data], data['Next_Close'], epochs=5, batch_size=32) # buatkan datanya"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnneW5_Skmpz",
        "outputId": "b5892dec-d71a-4f86-9dbe-21928da41b56"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "8/8 [==============================] - 9s 7ms/step - loss: 0.4522\n",
            "Epoch 2/5\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.3690\n",
            "Epoch 3/5\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.2893\n",
            "Epoch 4/5\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.2171\n",
            "Epoch 5/5\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.1630\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1d1b8df220>"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#predict\n",
        "predictions = model.predict([open_data, close_data, high_data, low_data, volume_data])\n",
        "\n",
        "#evaluate\n",
        "error = model.evaluate([open_data, close_data, high_data, low_data, volume_data], open_data)\n",
        "#error = model.evaluate([open_data, close_data, high_data, low_data, volume_data], data['Close'])\n",
        "print(\"Absolute Mean Error: \", error)\n",
        "\n",
        "\n",
        "# inverting data kembali ke nilai asli\n",
        "predictions = scaler.inverse_transform(predictions) # mungkin 2D\n",
        "#train_data   = scaler.inverse_transform(train_data) # beda dimensi error\n",
        "\n",
        "#X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1)) #hapus\n",
        "train_data=train_data.reshape(-1, 1)\n",
        "train_data   = scaler.inverse_transform(train_data)\n",
        "\n",
        "#test_data   = scaler.inverse_transform(test_data)\n",
        "#testPredict = scaler.inverse_transform(testPredict)                       #Error\n",
        "#testY = scaler.inverse_transform(testY)     \n",
        "\n",
        "#plot\n",
        "plt.plot(predictions, label='Predictions')\n",
        "#plt.plot(data['Next_Close'], label='Actual')\n",
        "#plt.plot(new_array1, label='Actual') # buatkan datanya\n",
        "plt.plot(train_data, label='Training') # buatkan datanya\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "K1tFvsntkppS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}